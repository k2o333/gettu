#!/usr/bin/env python3
"""
æ•°æ®å®Œæ•´æ€§æ£€æŸ¥è„šæœ¬
éªŒè¯ config.py ä¸­é…ç½®çš„æ•°æ®æ˜¯å¦å®Œæ•´
"""
import polars as pl
import logging
from pathlib import Path
from datetime import datetime
from config import DATA_INTERFACE_CONFIG, ROOT_DIR, PartitionGranularity


def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )


def check_data_integrity(data_type=None):
    """æ£€æŸ¥æŒ‡å®šæ•°æ®ç±»å‹çš„æ•°æ®å®Œæ•´æ€§"""
    if data_type:
        if data_type not in DATA_INTERFACE_CONFIG:
            print(f"é”™è¯¯: æ•°æ®ç±»å‹ '{data_type}' ä¸å­˜åœ¨äºé…ç½®ä¸­")
            return False
        data_types_to_check = [data_type]
    else:
        data_types_to_check = list(DATA_INTERFACE_CONFIG.keys())

    print(f"å¼€å§‹æ£€æŸ¥ {len(data_types_to_check)} ä¸ªæ•°æ®ç±»å‹çš„å®Œæ•´æ€§...")
    print("=" * 80)
    
    all_checks_passed = True
    
    for dt in data_types_to_check:
        config = DATA_INTERFACE_CONFIG[dt]
        storage_path = config['storage']['path']
        partition_granularity = config['storage']['partition_granularity']
        
        print(f"\næ£€æŸ¥æ•°æ®ç±»å‹: {dt}")
        print(f"  å­˜å‚¨è·¯å¾„: {storage_path}")
        print(f"  åˆ†åŒºç²’åº¦: {partition_granularity}")
        
        try:
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
            if partition_granularity != PartitionGranularity.NONE:
                # åˆ†åŒºå­˜å‚¨ - ä½†å®é™…æ£€æŸ¥æ—¶ä¹Ÿè¦è€ƒè™‘å¯èƒ½å­˜å‚¨ä¸ºéåˆ†åŒºæ–‡ä»¶çš„æƒ…å†µ
                if storage_path.exists():
                    # æ£€æŸ¥æ˜¯å¦ç›´æ¥æ˜¯.parquetæ–‡ä»¶ï¼ˆéåˆ†åŒºå­˜å‚¨ï¼‰
                    parquet_files = list(storage_path.glob("**/*.parquet"))
                    parquet_files = [f for f in parquet_files if f.name.endswith('.parquet')]

                    # åŒæ—¶æ£€æŸ¥åˆ†åŒºæ ¼å¼ï¼ˆå¦‚ year=2023/data.parquetï¼‰
                    partition_files = list(storage_path.glob("**/data.parquet"))

                    if partition_files:  # æœ‰åˆ†åŒºæ ¼å¼æ–‡ä»¶
                        data_files = partition_files
                        storage_type = "åˆ†åŒºæ ¼å¼"
                    elif parquet_files:  # æœ‰ç›´æ¥çš„.parquetæ–‡ä»¶
                        data_files = parquet_files
                        storage_type = "éåˆ†åŒºæ–‡ä»¶"
                    else:  # éƒ½æ²¡æœ‰
                        print(f"  âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ (æœŸæœ›åˆ†åŒºå­˜å‚¨)")
                        all_checks_passed = False
                        continue

                    total_records = 0
                    for file_path in data_files:
                        try:
                            df = pl.read_parquet(file_path)
                            total_records += len(df)
                        except Exception as e:
                            print(f"  âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
                            all_checks_passed = False
                            continue

                    print(f"  âœ… æ‰¾åˆ° {len(data_files)} ä¸ª{storage_type}ï¼Œæ€»è®¡ {total_records} æ¡è®°å½•")

            else:
                # éåˆ†åŒºå­˜å‚¨
                if storage_path.suffix != '.parquet':
                    storage_path = Path(f"{storage_path}.parquet")

                if not storage_path.exists():
                    print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {storage_path}")
                    all_checks_passed = False
                    continue

                try:
                    df = pl.read_parquet(storage_path)
                    print(f"  âœ… æ–‡ä»¶å­˜åœ¨ï¼Œè®°å½•æ•°: {len(df)}")

                    # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦å­˜åœ¨
                    partition_field = config['storage']['partition_field']
                    if partition_field and partition_field in df.columns:
                        print(f"  âœ… åˆ†åŒºå­—æ®µ '{partition_field}' å­˜åœ¨ï¼Œæœ‰ {df[partition_field].n_unique()} ä¸ªå”¯ä¸€å€¼")

                    # æ˜¾ç¤ºæ•°æ®æ—¥æœŸèŒƒå›´ï¼ˆå¦‚æœæœ‰æ—¥æœŸå­—æ®µï¼‰
                    date_fields = [col for col in df.columns if 'date' in col.lower()]
                    if date_fields:
                        for date_field in date_fields[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ—¥æœŸå­—æ®µ
                            try:
                                date_min = df[date_field].min()
                                date_max = df[date_field].max()
                                print(f"  ğŸ“… {date_field} èŒƒå›´: {date_min} ~ {date_max}")
                            except:
                                continue

                except Exception as e:
                    print(f"  âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {storage_path}: {str(e)}")
                    all_checks_passed = False
                    continue
        
        except Exception as e:
            print(f"  âŒ æ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            all_checks_passed = False
        
        print("-" * 60)
    
    print(f"\nå®Œæ•´æ€§æ£€æŸ¥å®Œæˆ!")
    print(f"æ€»ä½“ç»“æœ: {'âœ… å…¨éƒ¨é€šè¿‡' if all_checks_passed else 'âŒ å­˜åœ¨é—®é¢˜'}")
    return all_checks_passed


def check_data_coverage_by_date_range():
    """æ£€æŸ¥æ•°æ®çš„æ—¶é—´è¦†ç›–èŒƒå›´"""
    print("\n" + "=" * 80)
    print("æ•°æ®æ—¶é—´è¦†ç›–èŒƒå›´æ£€æŸ¥")
    print("=" * 80)
    
    date_coverage = {}
    
    for dt, config in DATA_INTERFACE_CONFIG.items():
        storage_path = config['storage']['path']
        partition_granularity = config['storage']['partition_granularity']
        
        # ç¡®ä¿è·¯å¾„æ˜¯ .parquet æ–‡ä»¶
        if partition_granularity == PartitionGranularity.NONE and storage_path.suffix != '.parquet':
            storage_path = Path(f"{storage_path}.parquet")
        
        try:
            if partition_granularity != PartitionGranularity.NONE:
                # åˆ†åŒºå­˜å‚¨ - éœ€è¦æŸ¥æ‰¾æ‰€æœ‰åˆ†åŒºä¸­çš„æ•°æ®
                if partition_granularity == PartitionGranularity.YEAR:
                    data_files = list(storage_path.glob("**/data.parquet"))
                elif partition_granularity == PartitionGranularity.YEAR_MONTH:
                    data_files = list(storage_path.glob("**/data.parquet"))
                else:
                    continue
                
                all_dates = []
                date_field = config['storage']['partition_field']
                
                for file_path in data_files:
                    try:
                        df = pl.read_parquet(file_path)
                        if date_field and date_field in df.columns:
                            dates = df[date_field].drop_nulls().cast(pl.Utf8).to_list()
                            all_dates.extend(dates)
                    except:
                        continue
                
                if all_dates:
                    all_dates = [d.replace('-', '').replace('/', '') for d in all_dates if d]
                    all_dates = [d for d in all_dates if len(d) >= 8]  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ—¥æœŸä¿¡æ¯
                    if all_dates:
                        date_coverage[dt] = {
                            'min_date': min(all_dates),
                            'max_date': max(all_dates),
                            'unique_count': len(set(all_dates))
                        }
            
            else:
                # éåˆ†åŒºå­˜å‚¨
                if storage_path.exists():
                    df = pl.read_parquet(storage_path)
                    date_field = config['storage']['partition_field']
                    
                    if date_field and date_field in df.columns:
                        dates = df[date_field].drop_nulls().cast(pl.Utf8).to_list()
                        dates = [d.replace('-', '').replace('/', '') for d in dates if d]
                        dates = [d for d in dates if len(d) >= 8]  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ—¥æœŸä¿¡æ¯
                        
                        if dates:
                            date_coverage[dt] = {
                                'min_date': min(dates),
                                'max_date': max(dates),
                                'unique_count': len(set(dates))
                            }
        
        except Exception:
            continue
    
    # æ‰“å°æ—¶é—´è¦†ç›–èŒƒå›´
    for dt, coverage in date_coverage.items():
        print(f"{dt:20} | {coverage['min_date']} ~ {coverage['max_date']} | {coverage['unique_count']} å¤©")


def main():
    setup_logging()
    
    print("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å·¥å…·")
    print("æ”¯æŒçš„å‘½ä»¤:")
    print("  python data_integrity_check.py                    # æ£€æŸ¥æ‰€æœ‰æ•°æ®ç±»å‹")
    print("  python data_integrity_check.py <data_type>        # æ£€æŸ¥æŒ‡å®šæ•°æ®ç±»å‹")
    
    import sys
    if len(sys.argv) > 1:
        data_type = sys.argv[1]
        check_data_integrity(data_type)
    else:
        check_data_integrity()
        check_data_coverage_by_date_range()


if __name__ == "__main__":
    main()